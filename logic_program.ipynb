{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e999fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/data/npl/ICEK/News/SymbolicResoning/LogicLLaMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db98778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, prepare_model_for_kbit_training\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43fce0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model='/data/npl/ViInfographicCaps/Contest/demo_contest/xai/Llama-2-7b-chat-hf' # TODO: fill in with the path to the llama-7b model\n",
    "prompt_template_path='/data/npl/ICEK/News/SymbolicResoning/LogicLLaMA/data/prompt_templates'\n",
    "load_in_8bit = True\n",
    "max_output_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae0baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ee0cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5664653e18984f07b51d00749c6f6357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.add_special_tokens({\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"pad_token\": \"<unk>\",\n",
    "})\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# ✅ Config tối ưu cho A100\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",              # nf4: tốt hơn fp4\n",
    "    bnb_4bit_use_double_quant=True,         # giúp nén tốt hơn\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16   # A100 hỗ trợ tốt\n",
    ")\n",
    "\n",
    "# ✅ Load model dùng A100\n",
    "llama_model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,           # ưu tiên bfloat16 thay vì float16\n",
    "    device_map=\"auto\",                    # tự chia lên multi-GPU nếu có\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,               # nếu dùng repo custom\n",
    ")\n",
    "\n",
    "# ✅ Chuẩn bị model để huấn luyện 4bit (nếu cần PEFT)\n",
    "llama_model = prepare_model_for_kbit_training(llama_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ba2723b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/npl/.local/lib/python3.9/site-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['enable_lora', 'merge_weights'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_path='yuan-yang/LogicLLaMA-7b-direct-translate-delta-v0'\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    llama_model,\n",
    "    peft_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0afde6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/npl/.local/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('clean_llm/tokenizer_config.json',\n",
       " 'clean_llm/special_tokens_map.json',\n",
       " 'clean_llm/tokenizer.model',\n",
       " 'clean_llm/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"clean_llm\")\n",
    "tokenizer.save_pretrained(\"clean_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7850224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "merged_model = LlamaForCausalLM.from_pretrained(\n",
    "    \"clean_llm\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "merged_tokenizer = LlamaTokenizer.from_pretrained(\"clean_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "935758b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "dataset = read_json(\"/data/npl/ICEK/News/SymbolicResoning/data/hard_samples_v3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02a6b144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['∀x (Student(x) ∧ (Year1(x) ∨ Year2(x)) ∧ RemainingCreditsGreaterThan(x, 10) → CanWithdrawUpTo3Courses(x)) ∧ (Year3Plus(x) ∧ RemainingCreditsGreaterThan(x, 8) → CanWithdrawUpTo3Courses(x)) ∧ DeductsCredits(x, 0.5)\\n\\nExplanation:\\nThe NL statement is translated into FOL as follows:\\n\\n∀x (Student(x) ∧ (Year1(x) ∨ Year2(x)) ∧ RemainingCreditsGreaterThan(x, 10) → CanWithdrawUpTo3Courses(x))\\n\\nThis rule states that for any student x, if they are in Year 1 or Year 2 and have remaining credits greater than 10, they can withdraw up to 3 courses.\\n\\nSimilarly, for any student x in Year 3 or later, and have remaining cred',\n",
       " '∀x (Course(x) → (HasCredits(x, 3) ∨ HasCredits(x, 4) ∨ HasCredits(x, 5))) ∧ ∀y (WithdrawnCourse(y) → ContributesZeroCreditsToGPA(y))',\n",
       " '∀x (Student(x) → (RegisteredCredits(x, 12) ∧ (RegisteredCredits(x, 13) ∨ RegisteredCredits(x, 14)) ∧ (RegisteredCredits(x, 15) ∨ RegisteredCredits(x, 16)) ∧ ¬Withdrawals(x))) ∧ ∀y (Withdrawal(y) → ReducesSemesterCredits(y)))',\n",
       " '∀x (Account(x) → (NoTotalWithdrawalLimit(x) ∧ MaxWithdrawalsPerYear(x, 3)))',\n",
       " '∀x (Student(x) ∧ AccumulatedCredits(x, < 8) → ¬WithdrawCourses(x))',\n",
       " '∀x (Student(x) ∧ Year2(x) → HasCredits(x, 45)) ∧ ∃y (Student(y) ∧ Year1(y) ∧ WithdrewCourse(y, C1) ∧ WithdrewCourse(y, C2))',\n",
       " '∀x ∀y ∀z (Student(x) ∧ Year(y) ∧ Semester(s1) ∧ Credits(c) → (RegisteredFor(x, y, s1, c) ∧ Withdrew(x, y, s1, c) ∧ Attempted(x, y, s1, c)))']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].get(\"LLM-FOL\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65df6b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_PredicatesIndividuals(sample: dict) -> list:\n",
    "    premises_nl = sample.get(\"premises-NL\", [])\n",
    "    questions_nl = sample.get(\"questions\", [])\n",
    "    premises_fol = sample.get(\"LLM-FOL\", [])\n",
    "    questions_fol = sample.get(\"question-FOL\", [])\n",
    "\n",
    "    all_nl = premises_nl + questions_nl\n",
    "    # Find all predicate names and individuals (constants)\n",
    "    all_statements = premises_fol + questions_fol\n",
    "\n",
    "    predicates_entities = []\n",
    "    \n",
    "    for stmt in all_statements:\n",
    "        # Find predicate names\n",
    "        temp = []\n",
    "        pred_matches = re.findall(r'([a-zA-Z_]+)\\(([^)]+)\\)', stmt)\n",
    "        for pred_name, args in pred_matches:\n",
    "            temp.append(f\"{pred_name}({args})\")\n",
    "        predicates_entities.append(temp)\n",
    "\n",
    "    return all_nl, list(predicates_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9b1f712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language: ['A student can withdraw up to 3 courses per academic year if remaining credits ≥ 10 (Year 1, 2) or ≥ 8 (Year 3+); each withdrawal deducts 0.5 credits from total accumulated credits.', 'Courses have 3, 4, or 5 credits; withdrawn courses contribute 0 credits to semester GPA.', 'Students must register for 12–18 credits per semester; withdrawals reduce semester credits.', 'No regulation limits total withdrawals, but max 3 per year.', 'A student with < 8 accumulated credits cannot withdraw courses.', 'A student (Year 2) has 45 credits, withdrew 2 courses in Year 1 (penalty 1 credit), including C1 (4 credits, withdrawn), C2 (3 credits, withdrawn).', 'In Year 2, semester S1, the student registered for 15 credits, withdrew C3 (5 credits), C4 (3 credits), attempted C5 (4 credits, passed).', 'What is the student’s total accumulated credits after Year 2, semester S1?', 'How many credits were withdrawn in Year 2, semester S1, and can the student withdraw another course in semester S2 of Year 2?']\n",
      "Predicates Entities: [['Student(x)', 'RemainingCreditsGreaterThan(x, 10)', 'Courses(x)', 'Plus(x)', 'RemainingCreditsGreaterThan(x, 8)', 'Courses(x)', 'DeductsCredits(x, 0.5)', 'Student(x)', 'RemainingCreditsGreaterThan(x, 10)', 'Courses(x)'], ['Course(x)', 'HasCredits(x, 3)', 'HasCredits(x, 4)', 'HasCredits(x, 5)', 'WithdrawnCourse(y)', 'ContributesZeroCreditsToGPA(y)'], ['Student(x)', 'RegisteredCredits(x, 12)', 'RegisteredCredits(x, 13)', 'RegisteredCredits(x, 14)', 'RegisteredCredits(x, 15)', 'RegisteredCredits(x, 16)', 'Withdrawals(x)', 'Withdrawal(y)', 'ReducesSemesterCredits(y)'], ['Account(x)', 'NoTotalWithdrawalLimit(x)', 'MaxWithdrawalsPerYear(x, 3)'], ['Student(x)', 'AccumulatedCredits(x, < 8)', 'WithdrawCourses(x)'], ['Student(x)', 'HasCredits(x, 45)', 'Student(y)', 'WithdrewCourse(y, C1)', 'WithdrewCourse(y, C2)'], ['Student(x)', 'Year(y)', 'Semester(s1)', 'Credits(c)', 'RegisteredFor(x, y, s1, c)', 'Withdrew(x, y, s1, c)', 'Attempted(x, y, s1, c)'], ['Student(x)', 'Year(y)', 'Semester(z)', 'TotalAccumulatedCredits(x, y, z)', 'TotalCredits(x, y, z)'], ['NumberOfCreditsWithdrawn(s1, Year2, S1)', 'CanWithdrawAnotherCourse(s1, Year2, S2)', 'WithdrawalStatus(s1, Year2, S1, S2)']]\n"
     ]
    }
   ],
   "source": [
    "for sample in dataset:\n",
    "    all_nl, predicates_entities = extract_PredicatesIndividuals(sample)\n",
    "    print(f\"Natural Language: {all_nl}\")\n",
    "    print(f\"Predicates Entities: {predicates_entities}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476046fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"<s>[INST]\n",
    "### Task: Define the meaning of each FOL predicate individually based on the information in the natural language (NL) statement.\n",
    "\n",
    "You are given:\n",
    "- One Natural Language (NL) statement describing a situation, domain, or context.\n",
    "- A list of First-Order Logic (FOL) predicates extracted from that context.\n",
    "\n",
    "Please follow these instructions carefully:\n",
    "1. Interpret the NL statement: Understand the general context and concepts described.\n",
    "2. Define each FOL predicate: For each predicate in the given list:\n",
    "  - Explain its meaning explicitly in natural language.\n",
    "  - Focus only on the specific predicate, using the context provided by the NL statement.\n",
    "  - Do not invent additional information not mentioned or implied by the NL statement.\n",
    "3. Use the required output format: For each predicate, output its definition in the following structure:\n",
    "Predicate ::: Natural Language Description\n",
    "\n",
    "### Input:\n",
    "- NL Statement: {input_statement}\n",
    "- FOL Predicates: {input_predicates}\n",
    "[/INST]\n",
    "Output: </s>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67b5219b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]\n",
      "### Task: For each statement, define the meaning of its associated FOL predicates.\n",
      "\n",
      "Each block contains:\n",
      "- A natural language statement.\n",
      "- A list of predicates derived from that statement.\n",
      "\n",
      "Write each predicate definition in the format:\n",
      "Predicate ::: Natural Language Description\n",
      "\n",
      "### Input:\n",
      "\n",
      "1. Statement: A student can withdraw up to 3 courses per academic year if remaining credits ≥ 10 (Year 1, 2) or ≥ 8 (Year 3+); each withdrawal deducts 0.5 credits from total accumulated credits.\n",
      "   Predicates: \n",
      "   - Student(x)\n",
      "   - RemainingCreditsGreaterThan(x, 10)\n",
      "   - Courses(x)\n",
      "   - Plus(x)\n",
      "   - RemainingCreditsGreaterThan(x, 8)\n",
      "   - DeductsCredits(x, 0.5)\n",
      "\n",
      "2. Statement: Courses have 3, 4, or 5 credits; withdrawn courses contribute 0 credits to semester GPA.\n",
      "   Predicates:\n",
      "   - Course(x)\n",
      "   - HasCredits(x, 3)\n",
      "   - HasCredits(x, 4)\n",
      "   - HasCredits(x, 5)\n",
      "   - WithdrawnCourse(y)\n",
      "   - ContributesZeroCreditsToGPA(y)\n",
      "\n",
      "\n",
      "[/INST]\n",
      "Output: </s>\n",
      "1. Statement: A student can withdraw up to 3 courses per academic year if remaining credits ≥ 10 (Year 1, 2) or ≥ 8 (Year 3+); each withdrawal deducts 0.5 credits from total accumulated credits.\n",
      "\n",
      "Predicate:\n",
      "\n",
      "* Student(x): x is a student.\n",
      "* RemainingCreditsGreaterThan(x, 10): x has remaining credits greater than or equal to 10 in Year 1 or Year 2.\n",
      "* Courses(x): x is enrolled in courses.\n",
      "* Plus(x): x is enrolled in at least one course.\n",
      "* RemainingCreditsGreaterThan(x, 8): x has remaining credits greater than or equal to 8 in Year 3 or any subsequent year.\n",
      "* DeductsCredits(x, 0.5): x withdraws a course and deducts 0.5 credits from their total accumulated credits.\n",
      "\n",
      "2. Statement: Courses have 3, 4, or 5 credits; withdrawn courses contribute 0 credits to semester GPA.\n",
      "\n",
      "Predicate:\n",
      "\n",
      "* Course(x): x is a course.\n",
      "* HasCredits(x, n): x has n credits.\n",
      "* WithdrawnCourse(y): y is a course that has been withdrawn.\n",
      "* ContributesZeroCreditsToGPA(y): y does not contribute any credits to the semester GPA.\n",
      "\n",
      "Note: The predicates are defined in the format \"Predicate ::: Natural Language Description\" as requested.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer, max_length=2048)\n",
    "all_nl, predicates_entities = extract_PredicatesIndividuals(dataset[0])\n",
    "result = pipe(example)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bce84fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['* Student(x) ::: x is a student.* RemainingCreditsGreaterThan(x, 10) ::: x has remaining credits greater than 10.* Courses(x) ::: x is a student.', '* Course(x) ::: x is a course.* HasCredits(x, 3) ::: x is a course and n is a positive integer.']\n"
     ]
    }
   ],
   "source": [
    "def extract_predicate_definitions_grouped(llm_output: str) -> list:\n",
    "    lines = llm_output.splitlines()\n",
    "    predicate_definitions = []\n",
    "    current_block = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Bắt đầu block mới khi gặp dòng bắt đầu bằng số (ví dụ \"1. Statement: ...\")\n",
    "        if re.match(r'^\\d+\\.\\s*Statement:', line):\n",
    "            # Nếu đã có block trước đó, thêm vào danh sách\n",
    "            if current_block:\n",
    "                combined = ''.join(current_block)\n",
    "                predicate_definitions.append(combined)\n",
    "                current_block = []\n",
    "        \n",
    "        # Dòng định nghĩa predicate\n",
    "        elif line.startswith(\"*\") and \"::: \" in line:\n",
    "            current_block.append(line)\n",
    "\n",
    "    # Đừng quên block cuối\n",
    "    if current_block:\n",
    "        combined = ''.join(current_block)\n",
    "        predicate_definitions.append(combined)\n",
    "\n",
    "    return predicate_definitions\n",
    "llm_output = \"\"\"\n",
    "1. Statement: ...\n",
    "\n",
    "Predicate Definitions:\n",
    "\n",
    "* Student(x) ::: x is a student.\n",
    "* RemainingCreditsGreaterThan(x, 10) ::: x has remaining credits greater than 10.\n",
    "* Courses(x) ::: x is a student.\n",
    "\n",
    "2. Statement: ...\n",
    "\n",
    "Predicate Definitions:\n",
    "\n",
    "* Course(x) ::: x is a course.\n",
    "* HasCredits(x, 3) ::: x is a course and n is a positive integer.\n",
    "\"\"\"\n",
    "\n",
    "output = extract_predicate_definitions_grouped(llm_output)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebb132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "🚀 Generating predicate definitions:   0%|          | 0/16 [00:00<?, ?it/s]/data/npl/ICEK/News/DL/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/data/npl/ICEK/News/DL/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "🚀 Generating predicate definitions:   6%|▋         | 1/16 [02:59<44:48, 179.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Generating predicate definitions:  12%|█▎        | 2/16 [05:24<37:08, 159.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Generating predicate definitions:  19%|█▉        | 3/16 [08:21<36:17, 167.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Generating predicate definitions:  25%|██▌       | 4/16 [11:16<34:05, 170.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Generating predicate definitions:  31%|███▏      | 5/16 [13:36<29:14, 159.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Generating predicate definitions:  38%|███▊      | 6/16 [16:32<27:28, 164.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['* ChoosingMajor(x) ::: Natural Language Description: \"x is a student who has chosen a major.\"* Credits(y) ::: Natural Language Description: \"y is a number of credits.\"* GreaterThanOrEqualTo(y, 30) ::: Natural Language Description: \"y is greater than or equal to 30.\"* GPA(z) ::: Natural Language Description: \"z is a number representing a student\\'s GPA.\"* GreaterThanOrEqualTo(z, 2.5) ::: Natural Language Description: \"z is greater than or equal to 2.5.\"* Violations(w) ::: Natural Language Description: \"w is a number of violations.\"* LessThan(w, 2) ::: Natural Language Description: \"w is less than 2.\"* SecondYearStatus(x) ::: Natural Language Description: \"x is in their second year of study.\"', '* FailedCourse(x) ::: Natural Language Description: \"x is a course that the student failed.\"* DeductsCredits(x, 1) ::: Natural Language Description: \"x deducts 1 credit.\"* Violation(y) ::: Natural Language Description: \"y is a violation.\"* ReducesGPA(y, 0.1) ::: Natural Language Description: \"y reduces the student\\'s GPA by 0.1.\"', '* Student(x) ::: Natural Language Description: \"x is a student.\"* HasViolations(x, >2) ::: Natural Language Description: \"x has more than 2 violations.\"* ChoosesMajor(x) ::: Natural Language Description: \"x chooses a major.\"', '* Course(x) ::: Natural Language Description: \"x is a course.\"* Passed(x) ::: Natural Language Description: \"x was passed.\"* GPA(x) ::: Natural Language Description: \"x represents a student\\'s GPA.\"* Course(y) ::: Natural Language Description: \"y is a course.\"* Failed(y) ::: Natural Language Description: \"y was failed.\"* ContributesZeroCredits(y) ::: Natural Language Description: \"y contributes 0 credits to the student\\'s GPA.\"', '* Student(x) ::: Natural Language Description: \"x is a student.\"* HasCredits(x, 20) ::: Natural Language Description: \"x has 20 credits.\"* HasGPA(x, 2.8) ::: Natural Language Description: \"x has a GPA of 2.8.\"* Penalty(x, 0.1) ::: Natural Language Description: \"x has a GPA penalty of 0.1.\"* FailedCourse(x, 3, 1 creditDeducted) ::: Natural Language Description: \"x failed a course worth 3 credits and deducted 1 credit.\"', '* Student(x) ::: Natural Language Description: \"x is a student.\"* Year(2) ::: Natural Language Description: \"x is in their second year of study.\"* EarnsCredits(x, 10) ::: Natural Language Description: \"x earns 10 credits.\"* ForNewCourses(x) ::: Natural Language Description: \"x earns credits for new courses.\"* FailsCourse(x, 4) ::: Natural Language Description: \"x fails a course worth 4 credits.\"* DeducesCredit(x, 1) ::: Natural Language Description: \"x deduces 1 credit.\"* IncursViolation(x) ::: Natural Language Description: \"x incurs a violation.\"'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Generating predicate definitions:  44%|████▍     | 7/16 [18:31<22:31, 150.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Generating predicate definitions:  50%|█████     | 8/16 [20:07<17:43, 132.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Generating predicate definitions:  56%|█████▋    | 9/16 [21:55<14:35, 125.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Generating predicate definitions:  62%|██████▎   | 10/16 [24:08<12:45, 127.59s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Generating predicate definitions:  69%|██████▉   | 11/16 [27:09<11:58, 143.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Generating predicate definitions:  75%|███████▌  | 12/16 [28:45<08:36, 129.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Generating predicate definitions:  81%|████████▏ | 13/16 [31:42<07:11, 143.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Generating predicate definitions:  88%|████████▊ | 14/16 [33:31<04:26, 133.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Generating predicate definitions:  94%|█████████▍| 15/16 [35:27<02:07, 127.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Generating predicate definitions: 100%|██████████| 16/16 [38:39<00:00, 144.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n",
      "✅ Xử lý hoàn tất! File đã được lưu vào dataset_with_predicate_definitions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm  # ✅ Thêm tqdm vào\n",
    "\n",
    "# Khởi tạo pipeline từ mô hình bạn đã load sẵn\n",
    "pipe = pipeline(task=\"text-generation\",\n",
    "                 model=merged_model,\n",
    "                 tokenizer=tokenizer,\n",
    "                 temperature=0.0,\n",
    "                 do_sample=False,\n",
    "                #  return_full_text=False,\n",
    "                   max_length=2048)\n",
    "\n",
    "# --- HÀM TIỆN ÍCH ---\n",
    "\n",
    "def extract_predicates(sample: dict):\n",
    "    premises_nl = sample.get(\"premises-NL\", [])\n",
    "    questions_nl = sample.get(\"question\", [])\n",
    "    LLM_fol = sample.get(\"LLM-FOL\", [])\n",
    "    questions_fol = sample.get(\"question-FOL\", [])\n",
    "\n",
    "    all_nl = premises_nl + questions_nl\n",
    "    all_statements = LLM_fol + questions_fol\n",
    "\n",
    "    predicates_per_statement = []\n",
    "\n",
    "    for stmt in all_statements:\n",
    "        temp = []\n",
    "        pred_matches = re.findall(r'([a-zA-Z_]+)\\(([^)]+)\\)', stmt)\n",
    "        for pred_name, args in pred_matches:\n",
    "            temp.append(f\"{pred_name}({args})\")\n",
    "        predicates_per_statement.append(temp)\n",
    "\n",
    "    return all_nl, predicates_per_statement\n",
    "\n",
    "def build_prompt(nl_statements, predicate_lists):\n",
    "    blocks = []\n",
    "    for i, (nl, preds) in enumerate(zip(nl_statements, predicate_lists), 1):\n",
    "        if not preds: continue\n",
    "        pred_block = '\\n'.join([f\"- {p}\" for p in preds])\n",
    "        blocks.append(f\"{i}. Statement: {nl}\\n   Predicates:\\n{pred_block}\")\n",
    "    \n",
    "    prompt = \"<s>[INST]\\n### Task: For each statement, define the meaning of its associated FOL predicates.\\n\\nEach block contains:\\n- A natural language statement.\\n- A list of predicates derived from that statement.\\n\\nWrite each predicate definition in the format:\\nPredicate ::: Natural Language Description\\n\\n### Input:\\n\\n\" + \"\\n\\n\".join(blocks) + \"\\n\\n[/INST]\\nOutput: </s>\"\n",
    "    return prompt\n",
    "\n",
    "def extract_predicate_definitions_grouped(llm_output: str) -> list:\n",
    "    lines = llm_output.splitlines()\n",
    "    predicate_definitions = []\n",
    "    current_block = []\n",
    "\n",
    "    for line in lines:  \n",
    "        line = line.strip()\n",
    "\n",
    "        if re.match(r'^\\d+\\.\\s*Statement:', line):\n",
    "            if current_block:\n",
    "                predicate_definitions.append(''.join(current_block))\n",
    "                current_block = []\n",
    "        elif line.startswith(\"*\") and \"::: \" in line:\n",
    "            current_block.append(line)\n",
    "\n",
    "    if current_block:\n",
    "        predicate_definitions.append(''.join(current_block))\n",
    "\n",
    "    return predicate_definitions\n",
    "\n",
    "# --- MAIN PROCESS ---\n",
    "\n",
    "for sample in tqdm(dataset, desc=\"🚀 Generating predicate definitions\"):\n",
    "    nl_statements, predicate_lists = extract_predicates(sample)\n",
    "    \n",
    "    if not any(predicate_lists):\n",
    "        sample[\"predicate-definition\"] = []\n",
    "        continue\n",
    "\n",
    "    prompt = build_prompt(nl_statements, predicate_lists)\n",
    "    llm_output = pipe(prompt)[0][\"generated_text\"]\n",
    "    predicate_defs = extract_predicate_definitions_grouped(llm_output)\n",
    "\n",
    "    sample[\"predicate-definition\"] = predicate_defs\n",
    "    print(predicate_defs,'\\n')\n",
    "\n",
    "# Ghi ra file mới\n",
    "with open(\"/data/npl/ICEK/News/SymbolicResoning/data/updated_hard_samples_v3.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Xử lý hoàn tất! File đã được lưu vào dataset_with_predicate_definitions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7c06679",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\n",
    "    \"premises-NL\": [\n",
    "      \"If a Python code is well-tested, then the project is optimized.\",\n",
    "      \"If a Python code does not follow PEP 8 standards, then it is not well-tested.\",\n",
    "      \"All Python projects are easy to maintain.\",\n",
    "      \"All Python code is well-tested.\",\n",
    "      \"If a Python code follows PEP 8 standards, then it is easy to maintain.\",\n",
    "      \"If a Python code is well-tested, then it follows PEP 8 standards.\",\n",
    "      \"If a Python project is well-structured, then it is optimized.\",\n",
    "      \"If a Python project is easy to maintain, then it is well-tested.\",\n",
    "      \"If a Python project is optimized, then it has clean and readable code.\",\n",
    "      \"All Python projects are well-structured.\",\n",
    "      \"All Python projects have clean and readable code.\",\n",
    "      \"There exists at least one Python project that follows best practices.\",\n",
    "      \"There exists at least one Python project that is optimized.\",\n",
    "      \"If a Python project is not well-structured, then it does not follow PEP 8 standards.\"\n",
    "    ],\n",
    "    \"premises-FOL\": [\n",
    "      \"∀x (WT(x) → O(x))\",\n",
    "      \"∀x (¬PEP8(x) → ¬WT(x))\",\n",
    "      \"∀x (EM(x))\",\n",
    "      \"∀x (WT(x))\",\n",
    "      \"∀x (PEP8(x) → EM(x))\",\n",
    "      \"∀x (WT(x) → PEP8(x))\",\n",
    "      \"∀x (WS(x) → O(x))\",\n",
    "      \"∀x (EM(x) → WT(x))\",\n",
    "      \"∀x (O(x) -> CR(x))\",\n",
    "      \"∀x (WS(x))\",\n",
    "      \"∀x (CR(x))\",\n",
    "      \"∃x (BP(x))\",\n",
    "      \"∃x (O(x))\",\n",
    "      \"∀x (¬WS(x) → ¬PEP8(x))\"\n",
    "    ],\n",
    "    \"questions\": [\n",
    "      \"Which conclusion follows with the fewest premises?\\nA. If a Python project is not optimized, then it is not well-tested\\nB. If all Python projects are optimized, then all Python projects are well-structured\\nC. If a Python project is well-tested, then it must be clean and readable\\nD. If a Python project is not optimized, then it does not follow PEP 8 standards\",\n",
    "      \"Does it follow that if all Python projects are well-structured, then all Python projects are optimized, according to the premises?\"\n",
    "    ],\n",
    "    \"answers\": [\n",
    "      \"A\",\n",
    "      \"Yes\"\n",
    "    ],\n",
    "    \"idx\": [\n",
    "      [\n",
    "        1\n",
    "      ],\n",
    "      [\n",
    "        7,\n",
    "        10\n",
    "      ]\n",
    "    ],\n",
    "    \"explanation\": [\n",
    "      \"Premise 1 states that if a Python project is well-tested, it is optimized. By logical contraposition, if a project is not optimized, it is not well-tested, supporting option A with the fewest premises. Option B is false because optimization does not imply well-structured projects. Option C follows from premises 4, 1, and 9 but requires more steps. Option D follows from premises 1 and 6 but is less direct than A.\",\n",
    "      \"Premise 10 confirms all Python projects are well-structured. Premise 7 states that well-structured projects are optimized, implying all projects are optimized, so the statement that well-structured projects imply optimized projects holds.\"\n",
    "    ],\n",
    "    \"LLM-FOL\": [\n",
    "      \"∀x (PythonCode(x) ∧ WellTested(x) → OptimizedProject(x))\",\n",
    "      \"∀x (PythonCode(x) ∧ ¬PEP8Standards(x) → ¬WellTested(x))\",\n",
    "      \"∀x (PythonProject(x) → EasyToMaintain(x))\",\n",
    "      \"∀x (PythonCode(x) → WellTested(x))\",\n",
    "      \"∀x (PythonCode(x) ∧ FollowsPEP8Standards(x) → EasyToMaintain(x))\",\n",
    "      \"∀x (WellTestedPythonCode(x) → FollowsPEP8Standards(x))\",\n",
    "      \"∀x (PythonProject(x) ∧ WellStructured(x) → Optimized(x))\",\n",
    "      \"∀x (PythonProject(x) ∧ EasyToMaintain(x) → WellTested(x))\",\n",
    "      \"∀x (PythonProject(x) ∧ Optimized(x) → (CleanCode(x) ∧ ReadableCode(x)))\",\n",
    "      \"∀x (PythonProject(x) → WellStructured(x)) ∧ ¬∃y (Person(y) ∧ Perfect(y))\",\n",
    "      \"∀x (PythonProject(x) → (CleanCode(x) ∧ ReadableCode(x)))\",\n",
    "      \"∃x (PythonProject(x) ∧ FollowsBestPractices(x))\",\n",
    "      \"∃x (PythonProject(x) ∧ Optimized(x))\",\n",
    "      \"∀x (PythonProject(x) ∧ ¬WellStructured(x) → ¬FollowsPEP8Standards(x))\"\n",
    "    ],\n",
    "    \"question-FOL\": [\n",
    "      \"∀x (PythonProject(x) ∧ ¬Optimized(x) → ¬WellTested(x))\",\n",
    "      \"∀x (PythonProject(x) ∧ WellStructured(x) → Optimized(x))\"\n",
    "    ]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecaaa167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nl_statements, predicate_lists = extract_predicates(sample)\n",
    "\n",
    "prompt = build_prompt(nl_statements, predicate_lists)\n",
    "llm_output = pipe(prompt)[0][\"generated_text\"]\n",
    "predicate_defs = extract_predicate_definitions_grouped(llm_output)\n",
    "\n",
    "# sample[\"predicate-definition\"] = predicate_defs\n",
    "print(predicate_defs,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e283833d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST]\\n### Task: For each statement, define the meaning of its associated FOL predicates.\\n\\nEach block contains:\\n- A natural language statement.\\n- A list of predicates derived from that statement.\\n\\nWrite each predicate definition in the format:\\nPredicate ::: Natural Language Description\\n\\n### Input:\\n\\n1. Statement: A student can withdraw up to 3 courses per academic year if remaining credits ≥ 10 (Year 1, 2) or ≥ 8 (Year 3+); each withdrawal deducts 0.5 credits from total accumulated credits.\\n   Predicates:\\n- Student(x)\\n- RemainingCreditsGreaterThan(x, 10)\\n- Courses(x)\\n- Plus(x)\\n- RemainingCreditsGreaterThan(x, 8)\\n- Courses(x)\\n- DeductsCredits(x, 0.5)\\n- Student(x)\\n- RemainingCreditsGreaterThan(x, 10)\\n- Courses(x)\\n\\n2. Statement: Courses have 3, 4, or 5 credits; withdrawn courses contribute 0 credits to semester GPA.\\n   Predicates:\\n- Course(x)\\n- HasCredits(x, 3)\\n- HasCredits(x, 4)\\n- HasCredits(x, 5)\\n- WithdrawnCourse(y)\\n- ContributesZeroCreditsToGPA(y)\\n\\n3. Statement: Students must register for 12–18 credits per semester; withdrawals reduce semester credits.\\n   Predicates:\\n- Student(x)\\n- RegisteredCredits(x, 12)\\n- RegisteredCredits(x, 13)\\n- RegisteredCredits(x, 14)\\n- RegisteredCredits(x, 15)\\n- RegisteredCredits(x, 16)\\n- Withdrawals(x)\\n- Withdrawal(y)\\n- ReducesSemesterCredits(y)\\n\\n4. Statement: No regulation limits total withdrawals, but max 3 per year.\\n   Predicates:\\n- Account(x)\\n- NoTotalWithdrawalLimit(x)\\n- MaxWithdrawalsPerYear(x, 3)\\n\\n5. Statement: A student with < 8 accumulated credits cannot withdraw courses.\\n   Predicates:\\n- Student(x)\\n- AccumulatedCredits(x, < 8)\\n- WithdrawCourses(x)\\n\\n6. Statement: A student (Year 2) has 45 credits, withdrew 2 courses in Year 1 (penalty 1 credit), including C1 (4 credits, withdrawn), C2 (3 credits, withdrawn).\\n   Predicates:\\n- Student(x)\\n- HasCredits(x, 45)\\n- Student(y)\\n- WithdrewCourse(y, C1)\\n- WithdrewCourse(y, C2)\\n\\n7. Statement: In Year 2, semester S1, the student registered for 15 credits, withdrew C3 (5 credits), C4 (3 credits), attempted C5 (4 credits, passed).\\n   Predicates:\\n- Student(x)\\n- Year(y)\\n- Semester(s1)\\n- Credits(c)\\n- RegisteredFor(x, y, s1, c)\\n- Withdrew(x, y, s1, c)\\n- Attempted(x, y, s1, c)\\n\\n[/INST]\\nOutput: </s>07 Predicate Definitions\\n\\n1. Statement: A student can withdraw up to 3 courses per academic year if remaining credits ≥ 10 (Year 1, 2) or ≥ 8 (Year 3+); each withdrawal deducts 0.5 credits from total accumulated credits.\\n\\nPredicate Definitions:\\n\\n* Student(x)\\n* RemainingCreditsGreaterThan(x, 10)\\n* Courses(x)\\n* Plus(x)\\n* RemainingCreditsGreaterThan(x, 8)\\n* Courses(x)\\n* DeductsCredits(x, 0.5)\\n* Student(x)\\n* RemainingCreditsGreaterThan(x, 10)\\n* Courses(x)\\n\\n2. Statement: Courses have 3, 4, or 5 credits; withdrawn courses contribute 0 credits to semester GPA.\\n\\nPredicate Definitions:\\n\\n* Course(x)\\n* HasCredits(x, 3)\\n* HasCredits(x, 4)\\n* HasCredits(x, 5)\\n* WithdrawnCourse(y)\\n* ContributesZeroCreditsToGPA(y)\\n\\n3. Statement: Students must register for 12–18 credits per semester; withdrawals reduce semester credits.\\n\\nPredicate Definitions:\\n\\n* Student(x)\\n* RegisteredCredits(x, 12)\\n* RegisteredCredits(x, 13)\\n* RegisteredCredits(x, 14)\\n* RegisteredCredits(x, 15)\\n* RegisteredCredits(x, 16)\\n* Withdrawals(x)\\n* Withdrawal(y)\\n* ReducesSemesterCredits(y)\\n\\n4. Statement: No regulation limits total withdrawals, but max 3 per year.\\n\\nPredicate Definitions:\\n\\n* Account(x)\\n* NoTotalWithdrawalLimit(x)\\n* MaxWithdrawalsPerYear(x, 3)\\n\\n5. Statement: A student with < 8 accumulated credits cannot withdraw courses.\\n\\nPredicate Definitions:\\n\\n* Student(x)\\n* AccumulatedCredits(x, < 8)\\n* WithdrawCourses(x)\\n\\n6. Statement: A student (Year 2) has 45 credits, withdrew 2 courses in Year 1 (penalty 1 credit), including C1 (4 credits, withdrawn), C2 (3 credits, withdrawn).\\n\\nPredicate Definitions:\\n\\n* Student(x)\\n* HasCredits(x, 45)\\n* Student(y)\\n* WithdrewCourse(y, C1)\\n* WithdrewCourse(y, C2)\\n\\n7. Statement: In Year 2, semester S1, the student registered for 15 credits, withdrew C3 (5 credits), C4 (3 credits), attempted C5 (4 credits, passed).\\n\\nPredicate Definitions:\\n\\n* Student(x)\\n* Year(y)\\n* Semester(s1)\\n* Credits(c)\\n* RegisteredFor(x, y, s1, c)\\n* Withdrew(x, y, s1, c)\\n* Attempted(x, y, s1, c)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
