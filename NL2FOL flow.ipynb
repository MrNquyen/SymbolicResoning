{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1302c2",
   "metadata": {},
   "source": [
    "# This notebook demonstrates how to do inference with LogicLLaMA and parse the text FOL rule into a syntax tree that can be used elsewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51477f3a",
   "metadata": {},
   "source": [
    "# INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec78c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "from peft import PeftModel, prepare_model_for_kbit_training\n",
    "from utils import TranslationDataPreparer\n",
    "from generatev2 import llama_batch_generate\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfad74df",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=r'Z:\\WORK\\LogicLLaMA\\Llama-2-7b-hf' # TODO: fill in with the path to the llama-7b model\n",
    "prompt_template_path=r'Z:\\WORK\\LogicLLaMA\\data\\prompt_templates'\n",
    "load_in_8bit = True\n",
    "max_output_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e46b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\WORK\\LogicLLaMA\\LogicLLaMA_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "z:\\WORK\\LogicLLaMA\\LogicLLaMA_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "z:\\WORK\\LogicLLaMA\\LogicLLaMA_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.15s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.padding_side = \"left\"# Allow batched inference\n",
    "tokenizer.add_special_tokens({\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"unk_token\": '<unk>',\n",
    "    \"pad_token\": '<unk>',\n",
    "})  \n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=1\n",
    ")\n",
    "\n",
    "llama_model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=load_in_8bit,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")\n",
    "llama_model = prepare_model_for_kbit_training(llama_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24715d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_abcd_pattern(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if `s` contains, in order, on separate lines:\n",
    "      - a line starting with \"A\" \n",
    "      - then a line starting with \"B\"\n",
    "      - then a line starting with \"C\"\n",
    "      - then a line starting with \"D\"\n",
    "    \"\"\"\n",
    "    # \n",
    "    # Explanation of the pattern:\n",
    "    #  \\nA[^\\n]*     – a newline + “A” + anything up to the next newline\n",
    "    #  \\nB[^\\n]*     – then newline + “B” + anything up to its newline\n",
    "    #  \\nC[^\\n]*     – likewise for “C”\n",
    "    #  \\nD[^\\n]*:    – then newline + “D” + anything, ending with a colon\n",
    "    #\n",
    "    pattern = r\"\\nA[^\\n]*\\nB[^\\n]*\\nC[^\\n]*\\nD[^\\n]*\"\n",
    "    return bool(re.search(pattern, s))\n",
    "def has_comma_and_pattern(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if `s` contains the substring \", and\" anywhere.\n",
    "    \"\"\"\n",
    "    # simple regex for a comma + space + \"and\"\n",
    "    pattern = r\", and\"\n",
    "    return bool(re.search(pattern, s))\n",
    "def split_question_options(s: str):\n",
    "    # Capture groups:\n",
    "    # 1: question (lazy up to the line before A)\n",
    "    # 2: text after \"A\"\n",
    "    # 3: text after \"B\"\n",
    "    # 4: text after \"C\"\n",
    "    # 5: text after \"D\" (colon is matched but not included)\n",
    "    capture = (\n",
    "        r\"^(.*?)\\r?\\n\"       # 1: question (anything up to first newline before A)\n",
    "        r\"A\\s*([^\\n]*)\\r?\\n\"  # 2: A-line content\n",
    "        r\"B\\s*([^\\n]*)\\r?\\n\"  # 3: B-line content\n",
    "        r\"C\\s*([^\\n]*)\\r?\\n\"  # 4: C-line content\n",
    "        r\"D\\s*([^\\n]*)\"      # 5: D-line content (colon out of capture)\n",
    "    )\n",
    "    m = re.search(capture, s, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        raise ValueError(\"Failed to parse question/options despite matching the pattern\")\n",
    "\n",
    "    question = m.group(1).strip()\n",
    "    opts = [m.group(i).strip() for i in range(2, 6)]\n",
    "    return [question, opts[0], opts[1], opts[2], opts[3]]\n",
    "def split_double_question(parts):\n",
    "    return parts.split(\", and\")\n",
    "def combine_double_question(parts):\n",
    "    return \"<q>\".join(parts)\n",
    "def combine_question_options(parts):\n",
    "    \"\"\"\n",
    "    Given a list of exactly five strings:\n",
    "      [question, optionA, optionB, optionC, optionD]\n",
    "    returns a single string formatted as:\n",
    "\n",
    "      question\n",
    "      A optionA\n",
    "      B optionB\n",
    "      C optionC\n",
    "      D optionD\n",
    "    \"\"\"\n",
    "    q, a, b, c, d = parts\n",
    "    return \"\\n\".join([\n",
    "        q.strip(),\n",
    "        f\"A {a.strip()}\",\n",
    "        f\"B {b.strip()}\",\n",
    "        f\"C {c.strip()}\",\n",
    "        f\"D {d.strip()}:\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc5875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_fill(fol_list, data_list, generate_fn):\n",
    "    \"\"\"\n",
    "    Repeatedly call `generate_fn` on any positions where fol_list[i] is None,\n",
    "    pulling the same NL inputs from data_list until no slots remain None.\n",
    "    \"\"\"\n",
    "    none_idxs = [i for i, v in enumerate(fol_list) if v is None]\n",
    "    while none_idxs:\n",
    "        print(f\"GOT NONE at positions: {none_idxs}\")\n",
    "        retry_input = [data_list[i] for i in none_idxs]\n",
    "        _, retry_parts = generate_fn(input_str=retry_input)\n",
    "        # retry_parts is a list of (inp_dict, fol_str)\n",
    "        for orig_idx, (_, new_fol) in zip(none_idxs, retry_parts):\n",
    "            fol_list[orig_idx] = new_fol\n",
    "        none_idxs = [i for i, v in enumerate(fol_list) if v is None]\n",
    "    return fol_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae89b99",
   "metadata": {},
   "source": [
    "# LogicLLaMA Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26402cb7",
   "metadata": {},
   "source": [
    "## INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546202f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=11008, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_path='LogicLLaMA-7b-direct-translate-delta-v0.1'\n",
    "model = PeftModel.from_pretrained(\n",
    "    llama_model,\n",
    "    peft_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2863fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:\\WORK\\LogicLLaMA\\data\\prompt_templates\\continuous_correct_prompt_template.json\n",
      "Z:\\WORK\\LogicLLaMA\\data\\prompt_templates\\correct_prompt_template.json\n",
      "Z:\\WORK\\LogicLLaMA\\data\\prompt_templates\\paraphrase_prompt_template.json\n",
      "Z:\\WORK\\LogicLLaMA\\data\\prompt_templates\\translate_prompt_template.json\n"
     ]
    }
   ],
   "source": [
    "data_preparer = TranslationDataPreparer(\n",
    "    prompt_template_path,\n",
    "    tokenizer,\n",
    "    False,\n",
    "    256 # just a filler number\n",
    ")\n",
    "\n",
    "prepare_input = partial(\n",
    "    data_preparer.prepare_input,\n",
    "    **{\"nl_key\": \"NL\"},\n",
    "    add_eos_token=False,\n",
    "    eval_mode=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "batch_simple_generate = partial(\n",
    "    llama_batch_generate,\n",
    "    llama_model=model,\n",
    "    data_preparer=data_preparer,\n",
    "    max_new_tokens=max_output_len,\n",
    "    generation_config=generation_config,\n",
    "    prepare_input=prepare_input,\n",
    "    return_tensors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3134e1a",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set your starting index here\n",
    "start_idx = 0 \n",
    "\n",
    "# Load your data\n",
    "with open(r'Z:\\WORK\\LogicLLaMA\\demo.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "output_path = r'Z:\\WORK\\LogicLLaMA\\demo_v2.json'\n",
    "\n",
    "# Only loop from the specified index\n",
    "for idx in tqdm(range(start_idx, len(data)), desc=\"Processing samples\"):\n",
    "    sample = data[idx]\n",
    "\n",
    "    premises = sample.get(\"premises-NL\", [])\n",
    "    raw_questions = sample.get(\"questions\", [])\n",
    "\n",
    "    # 1) Flatten questions, record where we expanded MCQ vs “, and”\n",
    "    flat_qs = []\n",
    "    mcq_positions = []      # list of (start_idx, option_count)\n",
    "    comma_and_positions = []  # list of start_idx\n",
    "\n",
    "    for q in raw_questions:\n",
    "        if has_abcd_pattern(q):\n",
    "            parts = split_question_options(q)\n",
    "            mcq_positions.append((len(flat_qs), len(parts)))\n",
    "            flat_qs.extend(parts)\n",
    "        elif has_comma_and_pattern(q):\n",
    "            parts = split_double_question(q)\n",
    "            comma_and_positions.append(len(flat_qs))\n",
    "            flat_qs.extend(parts)\n",
    "        else:\n",
    "            flat_qs.append(q)\n",
    "\n",
    "    # 2) Build data_list in one go\n",
    "    data_list = (\n",
    "        [{\"NL\": p} for p in premises] +\n",
    "        [{\"NL\": q} for q in flat_qs]\n",
    "    )\n",
    "    sep_idx = len(premises)\n",
    "\n",
    "    # 3) Generate and retry‐fill\n",
    "    full_str, resp_parts = batch_simple_generate(input_str=data_list)\n",
    "    llm_fol = [fol for _, fol in resp_parts]\n",
    "    llm_fol = retry_fill(llm_fol, data_list, batch_simple_generate)\n",
    "\n",
    "    # 4) Slice out premises vs question‐FOL\n",
    "    sample['LLM-FOL'] = llm_fol[:sep_idx]\n",
    "    ques_fol = llm_fol[sep_idx:]\n",
    "\n",
    "    # 5) Combine back in **reverse** order so earlier splices don't shift later ones\n",
    "    for start, count in sorted(mcq_positions, reverse=True):\n",
    "        slice_ = ques_fol[start:start+count]\n",
    "        merged = combine_question_options(slice_)\n",
    "        ques_fol[start:start+count] = [merged]\n",
    "\n",
    "    for start in sorted(comma_and_positions, reverse=True):\n",
    "        slice_ = ques_fol[start:start+2]\n",
    "        merged = combine_double_question(slice_)\n",
    "        ques_fol[start:start+2] = [merged]\n",
    "\n",
    "    sample['question-FOL'] = ques_fol\n",
    "\n",
    "    # Save progress after each sample\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LogicLLaMA_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
